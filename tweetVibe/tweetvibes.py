# -*- coding: utf-8 -*-
"""TweetVibes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ginXDDxY3OXyk8pkCoKcQyDzfDU4U6bo
"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TweetVibes").config("spark.driver.host", "localhost") .config("spark.driver.memory", "2g").getOrCreate()

# mount googel drive

# df = spark.read.csv("/content/drive/MyDrive/dataset Tweet.csv")
# spark.read.csv()"/content/drive/MyDrive/cdac/tweetAnalysis_team2.csv")

df = spark.read.option("header","false").option("encoding","ISO-8859-1").option("delimeter",",").csv("tweetAnalysis_team2.csv")

df.show()

df = df.withColumnRenamed("_c0", "target").withColumnRenamed("_c1","id").withColumnRenamed("_c2","date").withColumnRenamed("_c3","flag").withColumnRenamed("_c4","user").withColumnRenamed("_c5","tweet")
#renamed column names

df.show()

# Get the number of rows in the DataFrame
num_rows = df.count()

# print("Number of rows in the DataFrame:", num_rows)
# #

#get number of columns (length of column names list)
num_columns = len(df.columns)

# print("Shape of the DataFrame: ({}, {})".format(num_rows, num_columns))

# to count unique values for specific column .
column_name = "target"

# to calculate number of unique values in specific column

num_uniq_values = df.select (column_name).distinct().count()

# print("Number of unique values in column '{}': {}".format(column_name, num_uniq_values))

# to calculate count of each unique value in specified column
value_counts = df.groupby(column_name).count()
# value_counts.show()

# value_counts.show()

# to select specific columns

data = df.select("tweet","target")

# data.show()

from pyspark.sql.functions import lower

# now converting "tweet" column in lowercase

# data_lower = data.withColumn("tweet",lower(data["tweet"]))

# data_lower.select('tweet').tail(5) # to find lower 5 tweet

import re
from pyspark.sql.functions import lower


# Convert "tweet" column to lowercase
data_lower = data.withColumn("cleaned_tweet", lower(data["tweet"]))

# Define regular expression pattern for cleaning
cleaning_pattern = r'@[A-Za-z0-9]+|https?:\/\/\S+|[#]+|RT[\s]+|[0-9]+|[^A-Za-z\s]+|\s+'

    # r'@[A-Za-z0-9]+',      # Remove mentions
    # r'[0-9]+',             # Remove numbers
    # r'#',                  # Remove hashtags
    # r'RT[\s]+',            # Remove retweet tags
    # r'https?:\/\/\S+',     # Remove hyperlinks
    # r'[^a-zA-Z\s]',        # Remove non-alphabetic characters (except spaces)
    # r'\s+',                # Replace multiple spaces with a single space

# Define a UDF to apply the cleaning pattern
def clean_text(text):
    return re.sub(cleaning_pattern, ' ', text)

# Register the UDF and apply it to the "cleaned_tweet" column
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

clean_text_udf = udf(clean_text, StringType())
data_lower = data_lower.withColumn("clean_tweet", clean_text_udf("cleaned_tweet"))

# Show the cleaned tweets
data_lower.select("clean_tweet").show(truncate=False)

data_lower.show()

# from pyspark.sql.functions import regexp_replace,col
# # Define regular expression patterns for cleaning
# mention_pattern = r'@[A-Za-z0-9]+'
# number_pattern = r'[0-9]+'
# hashtag_pattern = r'#'
# rt_pattern = r'RT[\s]+'
# hyperlink_pattern = r'https?:\/\/\S+'

# # Apply text cleaning using PySpark functions
# data_lower = data_lower.withColumn("tweet", regexp_replace(col("tweet"), "[^\w\s]", ""))
# data_lower = data_lower.withColumn("tweet", regexp_replace("tweet", mention_pattern, ""))
# data_lower = data_lower.withColumn("tweet", regexp_replace("tweet", number_pattern, ""))
# data_lower = data_lower.withColumn("tweet", regexp_replace("tweet", hashtag_pattern, ""))
# data_lower = data_lower.withColumn("tweet", regexp_replace("tweet", rt_pattern, ""))
# data_lower = data_lower.withColumn("tweet", regexp_replace("tweet", hyperlink_pattern, ""))
# data_lower = data_lower.withColumn("tweet", regexp_replace("tweet", r'(\W)(\1{2,})', r'\1'))

# Show the cleaned tweets
# data_lower.select("tweet").show(truncate=False)

# data_lower.show()



from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
from nltk.stem import WordNetLemmatizer
import nltk

# Download NLTK resources
nltk.download('wordnet')

"""Tokenizing Data"""

from pyspark.ml import Pipeline
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover

from pyspark.ml.feature import Tokenizer, StopWordsRemover
# Tokenize the text column
tokenizer = Tokenizer(inputCol="clean_tweet", outputCol="tweet_tok")
data_tokenized = tokenizer.transform(data_lower)

data_tokenized.show()

from pyspark.ml.feature import StopWordsRemover

# Remove stop words from the tokenized words
stopwords_remover = StopWordsRemover(inputCol="tweet_tok", outputCol="tweet_st")
data_filtered = stopwords_remover.transform(data_tokenized)
data_filtered.show()

from pyspark.sql.functions import udf

from pyspark.sql.types import ArrayType, StringType

from nltk.stem import WordNetLemmatizer
import nltk

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# # Broadcast the lemmatizer to worker nodes
# broadcasted_lemmatizer = spark.sparkContext.broadcast(lemmatizer)

# # Define a UDF for lemmatization
# def lemmatize_words_udf(words):
#     lemmatizer = broadcasted_lemmatizer.value
#     lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
#     return lemmatized_words

# Define a UDF for lemmatization
def lemmatize_words(words):
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return lemmatized_words

# Create a UDF for lemmatization
lemmatize_udf = udf(lemmatize_words, ArrayType(StringType()))

# Apply lemmatization using the UDF
data_lemmatized = data_filtered.withColumn("tweet_lem", lemmatize_udf("tweet_st"))

data_lemmatized.show()

#Specify the list of columns  to delete
columns_to_delete = ["tweet","cleaned_tweet","clean_tweet","tweet_tok","tweet_st"]

#Drop the specified columns from the DataFrame
new_data = data_lemmatized.drop(*columns_to_delete)

new_data.show()

from pyspark.sql.functions import col, sum

# Check for null values in each column
null_counts = new_data.select([sum(col(c).isNull().cast("int")).alias(c) for c in new_data.columns])

# Show the count of null values for each columnc
null_counts.show()

# Show the resulting DataFrame without the deleted columns
new_data.show(truncate=False)

from pyspark.ml.feature import HashingTF, IDF
from pyspark.ml import Pipeline
import joblib

# Initialize the HashingTF and IDF transformers
hashing_tf = HashingTF(inputCol="tweet_lem", outputCol="tf_features")

# Save the HashingTF transformer to a directory

idf = IDF(inputCol="tf_features", outputCol="tfidf_features")

# idf.save("idf_model")

pipeline = Pipeline(stages=[hashing_tf, idf])

# Fit and transform the data using the pipeline
model = pipeline.fit(new_data)

# from google.colab import drive
# drive.mount('/content/drive')

# model.save("model_Hash&Idf")

transformed_data = model.transform(new_data)
# model.save(path="/content/drive/MyDrive/cdac/pipeline_hash_idf")

# # Load the saved SVM model
# loaded_svm_model = LinearSVCModel.load("/content/SVM_Model.pickle")

# # Make predictions using the loaded SVM model
# predictions = loaded_svm_model.transform(d)

# Show the resulting DataFrame with TF-IDF features
transformed_data.select("target", "tfidf_features").show(truncate=False)

from pyspark.sql.functions import expr
from pyspark.ml.feature import StringIndexer

# Assuming your 'target' column contains string labels
indexer = StringIndexer(inputCol="target", outputCol="label")
indexed_data = indexer.fit(transformed_data).transform(transformed_data)

indexed_data.show()

# Specify the list of columns you want to delete
columns_to_delete = ["target","tweet_lem","tf_features"]
indexed_data = indexed_data.drop(*columns_to_delete)

print("Total number of rows in indexed_data:", indexed_data.count())

indexed_data.show()

indexed_data.printSchema()

# Split the data into training and testing sets
train_data, test_data = indexed_data.randomSplit([0.9, 0.1], seed=123)

# Assuming your DataFrame is named 'data'
distinct_values = train_data.select("label").distinct()

# Show distinct values
distinct_values.show()

train_data.show(truncate=False)

test_data.show()

train_data.select("tfidf_features").tail(5)

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Initialize the Linear Regression model
lr_model = LogisticRegression(featuresCol="tfidf_features", labelCol="label")

# Fit the model on the training data
lr_model = lr_model.fit(train_data)

# Make predictions on the test data
predictions = lr_model.transform(test_data)

# Evaluate the model using MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="label", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Accuracy:", accuracy)

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# Define a grid of hyperparameters
param_grid = ParamGridBuilder() \
    .addGrid(lr_model.regParam, [0.01, 0.1, 1.0]) \
    .addGrid(lr_model.elasticNetParam, [0.0, 0.5, 1.0]) \
    .build()

# Set up cross-validation
crossval = CrossValidator(estimator=lr_model,
                          estimatorParamMaps=param_grid,
                          evaluator=MulticlassClassificationEvaluator(metricName="accuracy"),
                          numFolds=5, seed=123)

# Fit and tune the model on training data
cv_model = crossval.fit(train_data)

# Get the best model from cross-validation
best_lr_model = cv_model.bestModel

# Evaluate the best model on test data
predictions = best_lr_model.transform(test_data)
accuracy = evaluator.evaluate(predictions)
print("Best Model Accuracy on Test Data:", accuracy)

from pyspark.ml.classification import NaiveBayes
# Initialize the NaiveBayes model
nb = NaiveBayes(featuresCol="tfidf_features", labelCol="label")

# Train the model
nb_model = nb.fit(train_data)

# Make predictions on the test data
predictions = nb_model.transform(test_data)

# Evaluate the model using MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="label", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Accuracy:", accuracy)

# !pip install mlflow

# !pip install --upgrade mlflow

# !pip show mlflow

# !pip install --upgrade pydantic

# from pyspark.ml.classification import NaiveBayes
# # Initialize the NaiveBayes model
# nb = NaiveBayes(featuresCol="tfidf_features", labelCol="label")

# # Train the model
# nb_model = nb.fit(train_data)

# # Make predictions on the test data
# predictions = nb_model.transform(test_data)

# # Evaluate the model using MulticlassClassificationEvaluator
# evaluator = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="label", metricName="accuracy")
# accuracy = evaluator.evaluate(predictions)
# print("Accuracy:", accuracy)

from pyspark.ml.classification import LinearSVC
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Initialize the LinearSVC (Support Vector Machine) model

svm = LinearSVC(featuresCol="tfidf_features", labelCol="label")

# Train the model
svm_model = svm.fit(train_data)

# Make predictions on the test data
predictions = svm_model.transform(test_data)

# Evaluate the model using MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="label", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Accuracy:", accuracy)

# svm.save("/content/drive/MyDrive/cdac/svm_model")

# from pyspark.ml.feature import HashingTF, IDF, StringIndexer
# from pyspark.ml.classification import LinearSVC
# from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# # Create a Spark session
# # spark = SparkSession.builder.appName("SVMExample").getOrCreate()

# # Create a synthetic dataset
# # data = [("positive", "This is a positive statement."),
# #         ("negative", "This is a negative statement."),
# #         ("positive", "I love this product!"),
# #         ("negative", "I'm really disappointed with the service."),
# #         ("positive", "Great experience with the team."),
# #         ("negative", "Terrible quality. Do not recommend.")]
# data = [("positive", "I love this product!"),
#         ("negative", "Terrible quality. Do not recommend.")]
# schema = ["label", "text"]
# df = spark.createDataFrame(data, schema)

# # Preprocessing: Tokenization, TF-IDF
# from pyspark.ml.feature import Tokenizer, StopWordsRemover

# tokenizer = Tokenizer(inputCol="text", outputCol="words")
# df = tokenizer.transform(df)

# hashing_tf = HashingTF(inputCol="words", outputCol="tf_features", numFeatures=100)
# idf = IDF(inputCol="tf_features", outputCol="tfidf_features")
# model = idf.fit(hashing_tf.transform(df))
# df = model.transform(hashing_tf.transform(df))

# # Label indexing
# indexer = StringIndexer(inputCol="label", outputCol="label_idx")
# indexed_df = indexer.fit(df).transform(df)

# # Split data into training and testing sets
# train_data, test_data = indexed_df.randomSplit([0.8, 0.2], seed=42)

# # Train the SVM model
# svm = LinearSVC(featuresCol="tfidf_features", labelCol="label_idx")
# svm_model = svm.fit(train_data)

# # Make predictions on the test data
# predictions = svm_model.transform(test_data)

# # Evaluate the model
# evaluator = MulticlassClassificationEvaluator(labelCol="label_idx", predictionCol="prediction", metricName="accuracy")
# accuracy = evaluator.evaluate(predictions)
# print("Accuracy:", accuracy)

# # Show predictions and true labels
# predictions.select("label", "prediction").show()

# from pyspark.ml.classification import LinearSVCModel

# d="I am happy"

# # Load the saved SVM model
# loaded_svm_model = LinearSVCModel.load("/content/SVM_Model.pickle")

# # Make predictions using the loaded SVM model
# predictions = loaded_svm_model.transform(d)

# # Load the HashingTF transformer from the saved directory
# loaded_hashing_tf = HashingTF.load("hashing_tf_model")
# # Save DataFrame as Parquet
# transformed_data.write.parquet("path_to_save/transformed_data.parquet")
# # Save DataFrame as JSON
# transformed_data.write.json("path_to_save/transformed_data.json")

# # Save the IDF transformer to a directory
# idf.save("idf_model")

# # Load the IDF transformer from the saved directory
# loaded_idf = IDF.load("idf_model")

# # Save the pipeline model to a directory
# model.save("pipeline_model")

# # Load the pipeline model from the saved directory
# loaded_model = PipelineModel.load("pipeline_model")